{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pdfminer.pdfpage import PDFPage - Python2\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser, PDFDocument, PDFNoOutlines\n",
    "from pdfminer.converter import PDFPageAggregator, TextConverter#, XMLConverter, HTMLConverter\n",
    "\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine #, LTFigure, LTImage\n",
    "c\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "\n",
    "import time\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfminer\n",
    "pdfminer.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below identify the probabilities of a text being written in a given language (using stop words). The language is taken as the max in languages_ratios- we want\n",
    "to keep English only (Africaans is assigned to Dutch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_languages(text):\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"My name's Anna.End.\")\n",
    "    ['My', name', 's', 'Anna', '.', 'End', '.']\n",
    "    '''\n",
    "    languages_ratios = {}\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    # number of unique stopwords appearing in analyzed text as included in nltk(Africaans classified as Dutch)\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in a given language,\n",
    "    returning the highest score and ratios\n",
    "    \"\"\"\n",
    "\n",
    "    ratios = get_languages(text)\n",
    "    \n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "\n",
    "    return most_rated_language, ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial pipeline\n",
    "\n",
    "### 1. information about a gazette can be found on the 1st page (vol, no, type etc) and header of the 2nd \n",
    "\n",
    "### 2. in order to pinpoint things to tag in a doc, we look up page numbers in what  could be called an 'outline'  (it is not as far as a doc structure is concerned, e.g. we cannot use .get_outlines())\n",
    "\n",
    "After reading the 1st/2nd page, we keep skimming through pages until we hit the 'outline'.\n",
    "This is a major hack, as accessing the table of contents (\"Outlines\") does not work for the gazettes (they are simply text boxes)\n",
    "http://www.unixuser.org/~euske/python/pdfminer/programming.html#layout\n",
    "\n",
    "We identify the page with an outline by looking up a variation of the following:\n",
    "\n",
    "GENERAL NOTICE / ALGEMENE KENNISGEWINGS\n",
    "CONTENT/Table of contents (note: undercover can be Table of ConTenTs, so to avoid confusion, we transform capital letters)\n",
    "\n",
    "tricky inconsistencies:\n",
    "- page number in the outline might not all be a separate box (can be a vector or extension of text)\n",
    "- page numbers are not aligned with the header (overlapp column labelled as 'gazette no')\n",
    "- page no. might appear at the end: 'table of contents' + entries + page no header, despite visually being ok. Not sure if it's the pdf or pdfminer's fault\n",
    "\n",
    "\n",
    "Idea for later:\n",
    "extend the PDFPageInterpreter and PDFDevice class in order to process them differently / obtain other information. \n",
    "\n",
    "### 3. going to the pages scraped from the outline and extracting English text/tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_classification_data(objstack_1, objstack_2):\n",
    "     \n",
    "        class_info = []\n",
    "        gazette_no = 0\n",
    "        # go over the front page\n",
    "        while objstack_1:\n",
    "            lt_obj=objstack_1.pop()\n",
    "            if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    text_obj = lt_obj.get_text().replace('\\n','').lower()\n",
    "                        #language, ratios = detect_language(text_obj)\n",
    "                    if ('gazette' in text_obj):\n",
    "\n",
    "                        class_info.append(text_obj)\n",
    "                        \n",
    "                    if ('vol' in text_obj): \n",
    "\n",
    "                        class_info.append(text_obj)\n",
    "                    \n",
    "                    if ('no.' in text_obj):  # might be repeated ot the only source of info\n",
    "\n",
    "                        class_info.append(text_obj)\n",
    "                        gazette_no = int(float(text_obj.split('.')[1].strip()))\n",
    "                        \n",
    "                    if ('province' in text_obj): \n",
    "\n",
    "                        class_info.append(text_obj)\n",
    "                    \n",
    "                    if ('issn' in text_obj):\n",
    "                        \n",
    "                        class_info.append(text_obj)\n",
    "                        \n",
    "        # header from the 2nd page          \n",
    "        for i in range(2):\n",
    "            lt_obj=objstack_2.pop()\n",
    "            if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    text_obj = lt_obj.get_text().replace('\\n','').lower()\n",
    "                    \n",
    "                    if ('no' in text_obj) and not ('no' in class_info):\n",
    "                        class_info.append(\"no. \" + text_obj.split('no.')[1].strip())\n",
    "                        continue\n",
    "                        \n",
    "                    for txt in text_obj.split(','):\n",
    "                        class_info.append(txt.strip())\n",
    "        \n",
    "        return class_info, gazette_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_outline(objstack):\n",
    "        text_obj=[]\n",
    "        start = 0 # when changed to 1, indicates the beg of the outline\n",
    "        \n",
    "        outline_obj = []\n",
    "        is_outline_page = 0\n",
    "        \n",
    "        # objects corresponding to the 'Page No' and 'Gazette No' \n",
    "        page_box = []\n",
    "        gazette_box = []\n",
    "        \n",
    "        while objstack:\n",
    "            lt_obj=objstack.pop()\n",
    "            \n",
    "            if isinstance(lt_obj, LTTextBox): #or isinstance(lt_obj, LTTextLine):\n",
    "                        text_obj = lt_obj.get_text().replace('\\n',' ').lower()\n",
    "                        \n",
    "                        #parse all below 'content':\n",
    "                        if ('contents' in text_obj) or ('provincial notices' in text_obj)\\\n",
    "                           or ('page no' in text_obj) or ('gazette no' in text_obj):\n",
    "                                start = 1\n",
    "                                is_outline_page = 1 # ind that this is the outline page\n",
    "                                \n",
    "                        if (start==1): # we are below 'contents' header now\n",
    "                           \n",
    "                            if ('page' in text_obj):\n",
    "                                page_box = lt_obj\n",
    "                                \n",
    "                            if ('gazette no' in text_obj):\n",
    "                                gazette_box = lt_obj\n",
    "                                \n",
    "                            outline_obj.append(lt_obj)               \n",
    "                \n",
    "        return is_outline_page, outline_obj, page_box, gazette_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pdf_to_gazette_classification(fp):\n",
    "    \n",
    "        parser = PDFParser(fp)\n",
    "        doc = PDFDocument()\n",
    "        parser.set_document(doc)\n",
    "        doc.set_parser(parser)\n",
    "        doc.initialize() # optional if passwo is there: pdf_pwd\n",
    "\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # to use in-built pdftotxt.py:\n",
    "        #device = TextConverter(rsrcmgr, outfp, laparams=LAParams()) \n",
    "        #process_pdf(rsrcmgr, device, fp) \n",
    "        #device.close() \n",
    "\n",
    "        text_parsed = [] # a list of strings, each representing text collected from each page of the doc\n",
    "\n",
    "        # find the number of pages- starts from 0\n",
    "        num_pages = max([i for i,page in enumerate(doc.get_pages())]) + 1\n",
    "\n",
    "        look_for_outline = 1\n",
    "\n",
    "        # find the page with content outline (not defined as outline in the doc, unfortnately)\n",
    "        for i, page in enumerate(doc.get_pages()):\n",
    "            try:\n",
    "                   interpreter.process_page(page)\n",
    "            except Exception as e:\n",
    "                    print(\"parsing of the first page is not possible\")\n",
    "                    continue\n",
    "            # receive the LTPage object for this page\n",
    "            #The layout from get_result() parses the strings into separate objects. \n",
    "            #These objects have several key components:type, the coordinates \n",
    "            #(startingx, startingy, endingx, endingy), and the content, e.g.\n",
    "            #<LTRect 258.000,39.720,297.000,51.000>\n",
    "            #Accessing the type: e.g. type(object)==LTRect)\n",
    "            \n",
    "            layout = device.get_result()\n",
    "\n",
    "            objstack=list(reversed(layout._objs))\n",
    "\n",
    "            # get the first page info: type of gazette, vol \n",
    "            if i==0:\n",
    "                objstack_1 = objstack\n",
    "                continue\n",
    "            \n",
    "            # header from the 2nd page (any 2+ page would do)\n",
    "            if i==1:\n",
    "                objstack_2 = objstack\n",
    "                classification_data = get_classification_data(objstack_1, objstack_2)\n",
    "                continue\n",
    "\n",
    "            if(look_for_outline):\n",
    "                # find page with the outline and extract outline text and pages it points to:\n",
    "                is_outline_page, outline_obj, page_box, gazette_box = get_outline(objstack)\n",
    "                if (is_outline_page):\n",
    "                    #print(is_outline_page, i)\n",
    "                    num_outline_page=i\n",
    "                    # we found the outline so move to extracting pages from there\n",
    "                    look_for_outline = 0\n",
    "                    pages_to_extract, notice_info = get_notice_pages(outline_obj, page_box, gazette_box, num_pages)\n",
    "                    break\n",
    "             #else:\n",
    "              #  print(look_for_outline)\n",
    "             #now we have to fetch the columns corresponding to the 'page no' or 'page' \n",
    "             #and extract the page numbers the main info is stored at\n",
    "        device.close()\n",
    "        \n",
    "        return classification_data, pages_to_extract, notice_info, num_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refers to the document itself:\n",
    "class Issue:\n",
    "  def __init__(self, publication = '', issn = 0, num_pages = 0, volume=0, gazette_title=''):\n",
    "    self.publication = publication # type of gazette\n",
    "    self.identifier = issn # ISSN code\n",
    "    self.page_range = num_pages # number of pages\n",
    "    self.edition_id = volume # volume\n",
    "    self.title = gazette_title # if extraordinary\n",
    "\n",
    "# refers to notices where the relevant data is stored: info stored in pages \n",
    "#referred to in the outline + extra info on the type of data and its place on the web \n",
    "\n",
    "class Document:\n",
    "  def __init__(self, page_range = [] , uri = 'http://www.gpwonline.co.za', media_type = 'text' ): \n",
    "    self.page_range = page_range\n",
    "    self.url = uri\n",
    "    self.media_type =  media_type # {text, jpg, ...} -> some notices are Figures   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we put the information into structured json template containing\n",
    "document identifiers of all sorts (classification of a gazette).\n",
    "\n",
    "Field ['subjects']: the entities that the notice is primarily about.\n",
    "\n",
    "Field ['about'] is for the content of the notice (parsed info).\n",
    "Depending on the type of document, it will either be \n",
    "\n",
    "- empty (if notices come as jpgs and cannot be processed), \n",
    "- csv/table for Liquor \n",
    "- tagged text for ... plain text\n",
    "\n",
    "\n",
    "Below we use json.dump to create a human readbale json format.\n",
    "However, for large no of files with large fileds and efficient way of saving data is:\n",
    "\n",
    "import jsonpickle\n",
    "json_obj = jsonpickle.encode(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_to_json(classification_data, pages_to_extract, notice_info, num_pages):\n",
    "        \"\"\"\n",
    "        classification_data: data about the document (date, vol, number, title)\n",
    "        pages_to_extract: where the info is\n",
    "        notice_info: what the notice is about, fetched from the 'outline'\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        classification = defaultdict(list) # dict to store data and dump into json\n",
    "\n",
    "        classification['issue'] = Issue() # instance of Issue\n",
    "        # publication = '', issn = 0, num_pages = 0, volume=0, notice_title='')\n",
    "\n",
    "        classification['document'] = Document() # instance of Document- mainly to gather\n",
    "           # pages where notices are published and their types\n",
    "\n",
    "        for x in set(classification_data):\n",
    "            if ('vol' in x):\n",
    "                uid_vol = x.split('.')[1].strip()\n",
    "\n",
    "                classification['issue'].edition_id = uid_vol\n",
    "\n",
    "            if ('no' in x):\n",
    "                uid_no = x.split('.')[1].strip()\n",
    "                classification['other_attributes'].append(uid_no) \n",
    "\n",
    "            if ('gazette' in x):\n",
    "                classification['issue'].publication = x\n",
    "                uid_type = x\n",
    "\n",
    "            if ('province' in x):\n",
    "                classification['other_attributes'].append(x)    \n",
    "\n",
    "            if ('extraordinary' in x):\n",
    "                classification['issue'].title = 'extraordinary'\n",
    "\n",
    "            if ('issn' in x):\n",
    "                classification['issue'].identifier = x\n",
    "            \n",
    "            #date\n",
    "            try:\n",
    "                \n",
    "                date_re = re.search(r'\\d{1,}[ ][A-Za-z]{3,}[ ]\\d{4}', x, re.IGNORECASE).string.split(' ')\n",
    "                classification['date_published'] = [date_re[1],date_re[2]]\n",
    "                classification['other_attributes'].append(date_re[0] + ' / ' + date_re[1] + ' / ' + date_re[2])\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # add info from the outline (keywords are important)\n",
    "        # shoulf be parsed: save keywords?\n",
    "        classification['summary'] = notice_info\n",
    "\n",
    "        # page_range, string /^[0-9]*(-[0-9]*)?$/\n",
    "        # The pages the document within the issue where to look for info\n",
    "\n",
    "        classification['document'].page_range =  pages_to_extract \n",
    "        classification['subjects'] = [] # entities\n",
    "        classification['about'] = [] # parsed info\n",
    "\n",
    "        classification['issue'].page_range = num_pages\n",
    "        \n",
    "        # timestamp of accessing the doc\n",
    "        classification['other_attributes'].append(time.asctime( time.localtime(time.time()) ))\n",
    "\n",
    "        #classification['source_url'] = \n",
    "\n",
    "        uid = uid_no + '_' + uid_vol\n",
    "        # modify id's if necessary\n",
    "        classification['uid'] = uid # must be unique\n",
    "        classification['identifier'] = uid  # + uid_type ? can be more descriptive possibly\n",
    "        \n",
    "        # see comment above this function\n",
    "        return json.dumps(classification, default=lambda o: o.__dict__, sort_keys=True)#, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_notice_pages(outline_obj, page_box, gazette_box, num_pages):\n",
    "        \n",
    "        notice_info = []\n",
    "        \n",
    "        # page numbers can be misaligned-> 'Gazette no.' and 'Page no' columns \n",
    "        # can appear in any order and be misaligned in headings (text shifted wrt to the heading)\n",
    "\n",
    "        page_contents = []\n",
    "       \n",
    "        for box in outline_obj:\n",
    "            text_obj = box.get_text().replace('\\n',' ').lower()\n",
    "            \n",
    "            #if ends with digit\n",
    "            pattern = re.search(r'\\d+$', text_obj)\n",
    "\n",
    "            ends_with_digit = [int(pattern.group()) if pattern else None]\n",
    "            \n",
    "            if (box.is_hoverlap(page_box)): \n",
    "                #https://github.com/euske/pdfminer/blob/master/pdfminer/layout.py\n",
    "                \n",
    "                if not ('page' in text_obj) and not ('no' in text_obj)\\\n",
    "                    and not ('..' in text_obj): \\\n",
    "                                                # check which ones overlap horizontally\n",
    "                                                # with the page box,\n",
    "                                                # discard the ones merged with text (dealt with below)\n",
    "                                                # and check whether we're not parsing the issue no\n",
    "                    print(text_obj)\n",
    "                    if (len(text_obj)>1): # sometimes page no's come as a vector, \n",
    "                                          # so we need to split the elements\n",
    "                        temp = text_obj.split(' ')\n",
    "                        for el in temp:\n",
    "                            if len(el)>0 and (int(float(el)) < num_pages):\n",
    "                                page_contents.append(int(float(el)))     \n",
    "\n",
    "                    else:   \n",
    "                        if int(text_obj) < num_pages:\n",
    "                            page_contents.append(int(text_obj))\n",
    "            \n",
    "            \n",
    "            if ('..' in text_obj) and (ends_with_digit != [None] ):\n",
    "                \n",
    "                t = text_obj.split('.')\n",
    "                last_el = t[-1].strip()\n",
    "                if last_el != '':\n",
    "                    x = int(float(last_el))\n",
    "\n",
    "                    if (x <= num_pages):  # sometimes gazette number parses in,need to see \n",
    "                                            # if it is a valid page no\n",
    "\n",
    "                        page_contents.append(x)\n",
    "                        \n",
    "                # save info about what can be found there:\n",
    "            if ('section' in text_obj) or ('act' in text_obj) or ('municipality' in text_obj)\\\n",
    "                or ('correction' in text_obj):\n",
    "                    notice_info.append(text_obj.replace(\"..\",\"\").strip())\n",
    "           \n",
    "        #unique pages\n",
    "        return sorted(list(set(page_contents))), notice_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_outline_nat_gov(objstack):\n",
    "        text_obj=[]\n",
    "        start = 0 # when changed to 1, indicates the beg of the outline\n",
    "        \n",
    "        outline_obj = []\n",
    "        is_outline_page = 0\n",
    "        \n",
    "        # objects corresponding to the 'Page No' and 'Gazette No' \n",
    "        page_box = []\n",
    "        gazette_box = []\n",
    "        \n",
    "        while objstack:\n",
    "            lt_obj=objstack.pop()\n",
    "            \n",
    "            if isinstance(lt_obj, LTTextBox): #or isinstance(lt_obj, LTTextLine):\n",
    "                        text_obj = lt_obj.get_text().replace('\\n',' ').lower()\n",
    "                        \n",
    "                        #parse all text boxes below 'content':\n",
    "                        if ('contents' in text_obj):\n",
    "                                start = 1\n",
    "                                is_outline_page = 1 # ind that this is the outline page\n",
    "                                \n",
    "                        if (start==1): # we are below 'contents' header now\n",
    "                           \n",
    "                            if ('page' in text_obj):\n",
    "                                page_box = lt_obj\n",
    "                                \n",
    "                            if ('gazette no' in text_obj):\n",
    "                                gazette_box = lt_obj\n",
    "                                \n",
    "                            outline_obj.append(lt_obj)               \n",
    "        # return objects containing Text on the page with the respective PageNo and Gazette Box    \n",
    "        return is_outline_page, outline_obj, page_box, gazette_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pdf_to_gazette_classification_nat_gov(fp):\n",
    "    \n",
    "    parser = PDFParser(fp)\n",
    "    doc = PDFDocument()\n",
    "    parser.set_document(doc)\n",
    "    doc.set_parser(parser)\n",
    "    doc.initialize() # optional if passwo is there: pdf_pwd\n",
    "\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    text_parsed = [] # a list of strings, each representing text collected from each page of the doc\n",
    "\n",
    "    # find the number of pages- starts from 0\n",
    "    num_pages = max([i for i,page in enumerate(doc.get_pages())]) + 1\n",
    "\n",
    "    look_for_outline = 1\n",
    "    outline_pages = []\n",
    "    classification_data = []\n",
    "    pages_to_extract = []\n",
    "    notice_info = []\n",
    "    gazette_no = 0\n",
    "    # find the page with content outline (not defined as outline in the doc, unfortnately)\n",
    "    for i, page in enumerate(doc.get_pages()):\n",
    "          \n",
    "        if i in range(5):# or i==5: # only 5th page  \n",
    "            try:\n",
    "                   interpreter.process_page(page)\n",
    "            except Exception as e:\n",
    "                    print(\"parsing of the first page is not possible\")\n",
    "                    continue\n",
    "         \n",
    "            layout = device.get_result()\n",
    "\n",
    "            objstack=list(reversed(layout._objs))\n",
    "\n",
    "            # get the first page info: type of gazette, vol \n",
    "            if i==0:\n",
    "                objstack_1 = objstack\n",
    "                continue\n",
    "            \n",
    "            # header from the 2nd page (any 2+ page would do)\n",
    "            if i==1:\n",
    "                objstack_2 = objstack\n",
    "                classification_data, gazette_no = get_classification_data(objstack_1, objstack_2)\n",
    "                continue\n",
    "            # is_outline_page says whether on current page there is data on relevant pages:\n",
    "            # returns text objects + page and gazette boxes to find numbers\n",
    "            is_outline_page, outline_obj, page_box, gazette_box = get_outline_nat_gov(objstack)\n",
    "            print(\"check \",i, is_outline_page)\n",
    "            if (is_outline_page and i < 10): # brute force stop searcg- outline is at the beg\n",
    "                    #print(is_outline_page, i)\n",
    "                    outline_pages.append(i)\n",
    "                    # we found the outline so move to extracting pages from there\n",
    "                    #look_for_outline = 0 \n",
    "                    pages_to_extract, notice_info = get_notice_pages_nat_gov(outline_obj, page_box, gazette_box, num_pages, gazette_no)\n",
    "                    break\n",
    "             \n",
    "    device.close()\n",
    "        \n",
    "    return classification_data, pages_to_extract, notice_info, num_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_notice_pages_nat_gov(outline_obj, page_box, gazette_box, num_pages, gazette_no):\n",
    "        \n",
    "        notice_info = []\n",
    "        \n",
    "        # page numbers can be misaligned-> 'Gazette no.' and 'Page no' columns \n",
    "        # can appear in any order and be misaligned in headings (text shifted wrt to the heading)\n",
    "        page_contents = []\n",
    "        for box in outline_obj:\n",
    "            text_obj = box.get_text().replace('\\n',' ').lower()\n",
    "            print(\"line\",text_obj)\n",
    "            #if ends with digit\n",
    "            pattern = re.search(r'\\d+$', text_obj)\n",
    "\n",
    "            ends_with_digit = [int(pattern.group()) if pattern else None]\n",
    "            #print(\"ob\",box)\n",
    "            #print(\"e\",ends_with_digit)\n",
    "            \n",
    "            if ( box.is_hoverlap(page_box) and (box.y1 < page_box.y1) ): \n",
    "\n",
    "                #https://github.com/euske/pdfminer/blob/master/pdfminer/layout.py\n",
    "                if not ('page' in text_obj) and not ('no' in text_obj)\\\n",
    "                    and not ('..' in text_obj):# and (ends_with_digit != [None] ): \\\n",
    "                                                # check which ones overlap horizontally\n",
    "                                                # with the page box,\n",
    "                                                # discard the ones merged with text (dealt with below)\n",
    "                                                # and check whether we're not parsing the issue no\n",
    "                    temp = text_obj.strip().split(' ')\n",
    "                    print('over  ', text_obj, 'tm ', temp)\n",
    "                    \n",
    "                    # FIX THAT FOR A BETTER CONDITION\n",
    "                    # sometimes page no's come as a vector and needs to be splitted\n",
    "                    if (len(temp)==2):  # when page no and gazette no come as a tuple\n",
    "                                        # or 2 page number come together \n",
    "                            \n",
    "                        elem1 = int(float(temp[0]))\n",
    "                        elem2 = int(float(temp[1]))\n",
    "                        print(gazette_no)\n",
    "                        if (elem1 < num_pages) and (elem2 == gazette_no): #page goes first\n",
    "                             page_contents.append(elem1) \n",
    "                             print('tem ', elem1)\n",
    "                        elif (elem2 < num_pages) and (elem1 == gazette_no): # elem2 is page \n",
    "                             page_contents.append(elem2)\n",
    "                            \n",
    "                        elif (elem2 < num_pages) and (elem1 == num_pages): # both are pages\n",
    "                            \n",
    "                             page_contents.append(elem1)\n",
    "                             page_contents.append(elem2)\n",
    "                            \n",
    "                    else:   # here we are probaly getting page no's as a vector\n",
    "                        for el in temp:\n",
    "                           print(el)\n",
    "                        #if len(el)>0 and (int(float(el)) < num_pages):\n",
    "                            #  page_contents.append(int(float(el.strip())))     \n",
    "\n",
    "                    #else:   \n",
    "                     #   if int(text_obj) < num_pages:\n",
    "                            #page_contents.append(int(text_obj))\n",
    "            \n",
    "    \n",
    "            # save info about what can be found there:\n",
    "            # IMPROVE BASED ON hoverlap\n",
    "            if ('section' in text_obj) or (' act' in text_obj) or ('municipality' in text_obj)\\\n",
    "                or ('correction' in text_obj):\n",
    "                    notice_info.append(text_obj.replace(\"..\",\"\").strip())\n",
    "           \n",
    "        #unique pages\n",
    "        return sorted(list(set(page_contents))), notice_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check  2 0\n",
      "check  3 0\n",
      "check  4 1\n",
      "line for purposes of reference, all proclamations, government  notices, general notices and board notices published are  included in the following table of contents which thus forms a  weekly index. let yourself be guided by the gazette numbers  in the righthand column: \n",
      "line alle proklamasies, goewermentskennisgewings, algemene  kennisgewings en raadskennisgewings gepubliseer, word vir  verwysingsdoeleindes in die volgende inhoudopgawe ingesluit  wat dus weeklikse indeks voorstel. laat uself deur die koerant- nommers in die regterhandse kolom lei: \n",
      "line weekly index \n",
      "line 39781 \n",
      "line weeklikse indeks \n",
      "line no. \n",
      "line page no. \n",
      "line gazette no. \n",
      "line no. \n",
      "line proclamation \n",
      "line 7 protection of constitutional democracy  against terrorist and related activities  act (33/2004) :notification by president in  respect of entities identified by the united  nations security council: section 25 ....... \n",
      "line 8 protection of constitutional democracy  against terrorist and related activities  act (33/2004) :notification by president in  respect of entities identified by the united  nations security council: section 25 ....... \n",
      "line 4 39737 \n",
      "over   4 39737  tm  ['4', '39737']\n",
      "39781\n",
      "line 13 39737 \n",
      "over   13 39737  tm  ['13', '39737']\n",
      "39781\n",
      "line proklamasies \n",
      "line 7 wet op die beskerming van konstitu- sionele demokrasie teen terroriste- en  verwante aktiwiteite (33/2004) :kennis- gewing deur president ten opsigte van en- titeite deur veiligheidsraad van die vereni- gde nasies geïdentifiseer : ar tikel 25 ........ \n",
      "line 8 wet op die beskerming van konstitu- sionele demokrasie teen terroriste- en  verwante aktiwiteite (33/2004) :kennis- gewing deur president ten opsigte van en- titeite deur veiligheidsraad van verenigde  nasies geïdentifiseer : ar tikel 25 ............... \n",
      "line bladsy no. \n",
      "line koerant no. \n",
      "line 6 \n",
      "line 39737 \n",
      "line 15 \n",
      "line 39737 \n",
      "line government notice \n",
      "line goewermentskennisgewings \n",
      "line arts and culture, department of \n",
      "line kuns en kultuur, departement van \n",
      "line 4 heraldry act (18/1962) :matriculation of a  heraldic representation ........................... \n",
      "line 14 39718 \n",
      "over   14 39718  tm  ['14', '39718']\n",
      "39781\n",
      "line 4 heraldiekwet (18/1962) :matrikulasie van  ’n heraldiese voorstelling ......................... \n",
      "line 14 \n",
      "line 39718 \n",
      "line justice and constitutional development, department of \n",
      "over   justice and constitutional development, department of  tm  ['justice', 'and', 'constitutional', 'development,', 'department', 'of']\n",
      "justice\n",
      "and\n",
      "constitutional\n",
      "development,\n",
      "department\n",
      "of\n",
      "line justisie en staatkundige ontwikkeling, departement van \n",
      "line r.3 rules board  for cour ts of law act  (107/1985) :amendment of the rules reg- ulating the conduct of the proceedings of  the several provincial and local divisions  of the high cour t of south africa .............. \n",
      "line 26 39715 \n",
      "over   26 39715  tm  ['26', '39715']\n",
      "39781\n",
      "line r.3 promotion of national unity and reconcil- iation act (34/1995) :regulations relating  to assistance to victims in respect of high- er education and training ......................... \n",
      "line 8 39742 \n",
      "over   8 39742  tm  ['8', '39742']\n",
      "39781\n",
      "line r.3 wet op reëlsraad vir geregshowe  (107/1985) :wysiging van die reëls waa- rby die verrigtings van die verskillende  provinsiale en plaaslike afdelings van die  hooggeregshof van suid-afrika gereël  word .......................................................... \n",
      "line r.3 wet op die bevordering van nasionale  eenheid en versoening (34/1995) :regu- lasies betreffende  bystand aan  slagof- fers ten opsigte van hoër onderwys en  opleiding .................................................. \n",
      "line 35 \n",
      "line 39715 \n",
      "line 10 \n",
      "line 39742 \n",
      "line rural development and land reform, department of \n",
      "over   rural development and land reform, department of  tm  ['rural', 'development', 'and', 'land', 'reform,', 'department', 'of']\n",
      "rural\n",
      "development\n",
      "and\n",
      "land\n",
      "reform,\n",
      "department\n",
      "of\n",
      "line 193 interim protection of informal land rights  act (31/1996) :extension of the application  for the provisions of the said act ............... \n",
      "line 4 39747 \n",
      "over   4 39747  tm  ['4', '39747']\n",
      "39781\n",
      "line landelike ontwikkeling en grondhervorming, departement  van \n",
      "line 193 wet op tussentydse beskerming van in- formele grondregte (31/1996) :verlenging  van die toepassing van die bepalings van  genoemde wet ......................................... \n",
      "line 5 \n",
      "line 39747 \n",
      "line south african revenue service \n",
      "line suid-afrikaanse inkomstediens \n",
      "line 191 income tax act (58/1962) :determination  of the daily amount in respect of meals  and incidental costs .................................. \n",
      "line 4 39724 \n",
      "over   4 39724  tm  ['4', '39724']\n",
      "39781\n",
      "line 191 inkomstebelastingwet (58/1962) :bepal- ing van dagtoelae ten opsigte van etes en  toevallige uitgawes vir doeleindes ............ \n",
      "line 11 \n",
      "line 39724 \n",
      "line 192 income tax act (58/1962) :fixing of rate  per kilometre in respect of motor vehicles  \n",
      "line 32 39724 \n",
      "over   32 39724  tm  ['32', '39724']\n",
      "39781\n",
      "line 192 inkomstebelastingwet (58/1962) :bepaling  van skaal per kilometer ten opsigte van  motorvoer tuie ........................................... \n",
      "line 35 \n",
      "line 39724 \n",
      "line transport, department of \n",
      "line vervoer, departement van \n",
      "line 197 south african national roads agency  limited and national roads act (7/1998)  :gauteng freeway improvement project,  toll roads: publication of tolls .................. \n",
      "line 4 39754 \n",
      "over   4 39754  tm  ['4', '39754']\n",
      "39781\n",
      "line 197 wet op suid-afrikaanse nasionale pad- agentskap beperk en nasionale paaie  (7/1998) :gauteng deurpad verbetering- sprojek, tolpaaie: publikasie van tol ......... \n",
      "line 20 \n",
      "line 39754 \n"
     ]
    }
   ],
   "source": [
    "# open the pdf file\n",
    "#fp = open('39823_18-3_NationalGovernment.pdf', 'rb')\n",
    "fp = open('39781_4-3_NationalGovernment.pdf', 'rb')\n",
    "\n",
    "classification_data, pages_extract, notice_info, num_pages = pdf_to_gazette_classification_nat_gov(fp)\n",
    "fp.close()\n",
    "\n",
    "#classification_json = save_to_json(classification_data, pages_extract, notice_info, num_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7 protection of constitutional democracy  against terrorist and related activities  act (33/2004) :notification by president in  respect of entities identified by the united  nations security council: section 25 .',\n",
       " '8 protection of constitutional democracy  against terrorist and related activities  act (33/2004) :notification by president in  respect of entities identified by the united  nations security council: section 25 .',\n",
       " '4 heraldry act (18/1962) :matriculation of a  heraldic representation .',\n",
       " 'r.3 rules board  for cour ts of law act  (107/1985) :amendment of the rules reg- ulating the conduct of the proceedings of  the several provincial and local divisions  of the high cour t of south africa',\n",
       " 'r.3 promotion of national unity and reconcil- iation act (34/1995) :regulations relating  to assistance to victims in respect of high- er education and training .',\n",
       " '193 interim protection of informal land rights  act (31/1996) :extension of the application  for the provisions of the said act .',\n",
       " '191 income tax act (58/1962) :determination  of the daily amount in respect of meals  and incidental costs',\n",
       " '192 income tax act (58/1962) :fixing of rate  per kilometre in respect of motor vehicles',\n",
       " '197 south african national roads agency  limited and national roads act (7/1998)  :gauteng freeway improvement project,  toll roads: publication of tolls']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notice_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp = open('3489_31-8_ECape.pdf', 'rb')\n",
    "# create a parser object associated with the file object\n",
    "parser = PDFParser(fp)\n",
    "# create a PDFDocument object that stores the document structure\n",
    "doc = PDFDocument()\n",
    "# connect the parser and document objects\n",
    "parser.set_document(doc)\n",
    "doc.set_parser(parser)\n",
    "doc.initialize() # optional if passwo is there: pdf_pwd\n",
    "\n",
    "rsrcmgr = PDFResourceManager()\n",
    "laparams = LAParams()\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "text_parsed = [] # a list of strings, each representing text collected from each page of the doc\n",
    "\n",
    "        # find the number of pages- starts from 0\n",
    "num_pages = max([i for i,page in enumerate(doc.get_pages())]) + 1\n",
    "\n",
    "look_for_outline = 1\n",
    "\n",
    "# try PDFPage.create_pages(doc) instead of doc.get_pages()\n",
    "for i, page in enumerate(doc.get_pages()):\n",
    "            \n",
    "            interpreter.process_page(page)\n",
    "                \n",
    "            layout = device.get_result()\n",
    "\n",
    "            objstack=list(reversed(layout._objs))\n",
    "\n",
    "            # get the first page info: type of gazette, vol \n",
    "            if i==0:\n",
    "                objstack_1 = objstack\n",
    "                continue\n",
    "            \n",
    "            # header from the 2nd page (any 2+ page would do)\n",
    "            if i==1:\n",
    "                objstack_2 = objstack\n",
    "                classification_data = get_classification_data(objstack_1, objstack_2)\n",
    "                continue\n",
    "\n",
    "            if(look_for_outline):\n",
    "                # find page with the outline and extract outline text and pages it points to:\n",
    "                is_outline_page, outline_obj, page_box, gazette_box = get_outline(objstack)\n",
    "                if (is_outline_page):\n",
    "                    #print(is_outline_page, i)\n",
    "                    num_outline_page=i\n",
    "                    # we found the outline so move to extracting pages from there\n",
    "                    look_for_outline = 0\n",
    "                    pages_to_extract, notice_info = get_notice_pages(page_box, gazette_box, num_pages)\n",
    "                    break\n",
    "             #else:\n",
    "              #  print(look_for_outline)\n",
    "             #now we have to fetch the columns corresponding to the 'page no' or 'page' \n",
    "             #and extract the page numbers the main info is stored at\n",
    "\n",
    "device.close()\n",
    "\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- detect whether it's a single or double column text\n",
    "- Extract just left column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing images:\n",
    "\n",
    "LTImage type: contains bits, colorspace, height,imagemask,name,srcsize,stream, and width.\n",
    "Note: PDFMiner seems work well only with jpegs ( apparently xpdf works with all images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if type(b) == LTImage:\n",
    "     imgbits=b.bits\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
