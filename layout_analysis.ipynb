{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pdfminer.pdfpage import PDFPage - Python2\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser, PDFDocument, PDFNoOutlines\n",
    "from pdfminer.converter import PDFPageAggregator, TextConverter#, XMLConverter, HTMLConverter\n",
    "\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine #, LTFigure, LTImage\n",
    "\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfminer\n",
    "pdfminer.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below identify the probabilities of a text being written in a given language (using stop words). The language is taken as the max in languages_ratios- we want\n",
    "to keep English only (Africaans is assigned to Dutch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_languages(text):\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"My name's Anna.End.\")\n",
    "    ['My', name', 's', 'Anna', '.', 'End', '.']\n",
    "    '''\n",
    "    languages_ratios = {}\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    # number of unique stopwords appearing in analyzed text as included in nltk(Africaans classified as Dutch)\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in a given language,\n",
    "    returning the highest score and ratios\n",
    "    \"\"\"\n",
    "\n",
    "    ratios = get_languages(text)\n",
    "    \n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "\n",
    "    return most_rated_language, ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial pipeline\n",
    "\n",
    "### 1. information about a gazette can be found on the 1st page (vol, no, type etc) and header of the 2nd \n",
    "\n",
    "### 2. in order to pinpoint things to tag in a doc, we look up page numbers in what  could be called an 'outline'  (it is not as far as a doc structure is concerned, e.g. we cannot use .get_outlines())\n",
    "\n",
    "After reading the 1st/2nd page, we keep skimming through pages until we hit the 'outline'.\n",
    "This is a major hack, as accessing the table of contents (\"Outlines\") does not work for the gazettes (they are simply text boxes)\n",
    "http://www.unixuser.org/~euske/python/pdfminer/programming.html#layout\n",
    "\n",
    "We identify the page with an outline by looking up a variation of the following:\n",
    "\n",
    "GENERAL NOTICE / ALGEMENE KENNISGEWINGS\n",
    "CONTENT/Table of contents (note: undercover can be Table of ConTenTs, so to avoid confusion, we transform capital letters)\n",
    "\n",
    "tricky inconsistencies:\n",
    "- page number in the outline might not all be a separate box (can be a vector or extension of text)\n",
    "- page numbers are not aligned with the header (overlapp column labelled as 'gazette no')\n",
    "- page no. might appear at the end: 'table of contents' + entries + page no header, despite visually being ok. Not sure if it's the pdf or pdfminer's fault\n",
    "\n",
    "\n",
    "Idea for later:\n",
    "extend the PDFPageInterpreter and PDFDevice class in order to process them differently / obtain other information. \n",
    "\n",
    "### 3. going to the pages scraped from the outline and extracting English text/tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_classification_data(objstack_1, objstack_2):\n",
    "     \n",
    "        class_info = []\n",
    "        # go over the front page\n",
    "        while objstack_1:\n",
    "            lt_obj=objstack_1.pop()\n",
    "            if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    text_obj = lt_obj.get_text().replace('\\n','').lower()\n",
    "                        #language, ratios = detect_language(text_obj)\n",
    "                    if ('gazette' in text_obj):\n",
    "\n",
    "                        class_info.append(text_obj)\n",
    "                        \n",
    "                    if ('vol' in text_obj): \n",
    "\n",
    "                        class_info.append(text_obj)\n",
    "                    \n",
    "                    if ('no.' in text_obj):  # might be repeated ot the only source of info\n",
    "\n",
    "                        class_info.append(text_obj)\n",
    "                    if ('province' in text_obj): \n",
    "\n",
    "                        class_info.append(text_obj)\n",
    "                    \n",
    "                    if ('issn' in text_obj):\n",
    "                        \n",
    "                        class_info.append(text_obj)\n",
    "                        \n",
    "        # header from the 2nd page          \n",
    "        for i in range(2):\n",
    "            lt_obj=objstack_2.pop()\n",
    "            if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    text_obj = lt_obj.get_text().replace('\\n','').lower()\n",
    "                    \n",
    "                    if ('no' in text_obj) and not ('no' in class_info):\n",
    "                        class_info.append(\"no. \" + text_obj.split('no.')[1].strip())\n",
    "                        continue\n",
    "                        \n",
    "                    for txt in text_obj.split(','):\n",
    "                        class_info.append(txt.strip())\n",
    "        \n",
    "        return class_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_outline(objstack):\n",
    "        text_obj=[]\n",
    "        start = 0 # when changed to 1, indicates the beg of the outline\n",
    "        \n",
    "        outline_obj = []\n",
    "        is_outline_page = 0\n",
    "        \n",
    "        # objects corresponding to the 'Page No' and 'Gazette No' \n",
    "        page_box = []\n",
    "        gazette_box = []\n",
    "        \n",
    "        while objstack:\n",
    "            lt_obj=objstack.pop()\n",
    "            \n",
    "            if isinstance(lt_obj, LTTextBox): #or isinstance(lt_obj, LTTextLine):\n",
    "                        text_obj = lt_obj.get_text().replace('\\n',' ').lower()\n",
    "                        \n",
    "                        #parse all below 'content':\n",
    "                        if ('contents' in text_obj) or ('provincial notices' in text_obj)\\\n",
    "                           or ('page no' in text_obj) or ('gazette no' in text_obj):\n",
    "                                start = 1\n",
    "                                is_outline_page = 1 # ind that this is the outline page\n",
    "                                \n",
    "                        if (start==1): # we are below 'contents' header now\n",
    "                           \n",
    "                            if ('page' in text_obj):\n",
    "                                page_box = lt_obj\n",
    "                                \n",
    "                            if ('gazette no' in text_obj):\n",
    "                                gazette_box = lt_obj\n",
    "                                \n",
    "                            outline_obj.append(lt_obj)               \n",
    "                \n",
    "        return is_outline_page, outline_obj, page_box, gazette_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_notice_pages(page_box, gazette_box, num_pages):\n",
    "        \n",
    "        notice_info = []\n",
    "        \n",
    "        # page numbers can be misaligned-> 'Gazette no.' and 'Page no' columns \n",
    "        # can appear in any order and be misaligned in headings (text shifted wrt to the heading)\n",
    "\n",
    "        page_contents = []\n",
    "       \n",
    "        for box in outline_obj:\n",
    "            text_obj = box.get_text().replace('\\n',' ').lower()\n",
    "\n",
    "            if (box.is_hoverlap(page_box)): \n",
    "                #https://github.com/euske/pdfminer/blob/master/pdfminer/layout.py\n",
    "\n",
    "                if not ('page' in text_obj) and not ('no' in text_obj)\\\n",
    "                    and not ('..' in text_obj): # check which ones overlap horizontally\n",
    "                                                # with the page box,\n",
    "                                                # discard the ones merged with text (dealt with below)\n",
    "                                                # and check whether we're not parsing the issue no\n",
    "                                    \n",
    "                    if (len(text_obj)>1): # sometimes page no's come as a vector, \n",
    "                                          # so we need to split the elements\n",
    "                        temp = text_obj.split(' ')\n",
    "                        for el in temp:\n",
    "                            if len(el)>0 and (int(float(el)) < num_pages):\n",
    "                                page_contents.append(int(float(el)))     \n",
    "\n",
    "                    else:   \n",
    "                        if int(text_obj) < num_pages:\n",
    "                            page_contents.append(int(text_obj))\n",
    "                 \n",
    "            if ('..' in text_obj):\n",
    "                \n",
    "                t = text_obj.split('.')\n",
    "                \n",
    "                last_el = t[-1].strip()\n",
    "                if last_el != '':\n",
    "                    x = int(float(last_el))\n",
    "\n",
    "                    if (x <= num_pages):  # sometimes gazette number parses in,need to see \n",
    "                                            # if it is a valid page no\n",
    "\n",
    "                        page_contents.append(x)\n",
    "                        \n",
    "                # save info about what can be found there:\n",
    "            if ('section' in text_obj) or ('act' in text_obj) or ('municipality' in text_obj)\\\n",
    "                or ('correction' in text_obj):\n",
    "                    notice_info.append(text_obj.replace(\"..\",\"\").strip())\n",
    "           \n",
    "        #unique pages\n",
    "        return sorted(list(set(page_contents))), notice_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pdf_to_gazette_classification(fp):\n",
    "    \n",
    "        parser = PDFParser(fp)\n",
    "        doc = PDFDocument()\n",
    "        parser.set_document(doc)\n",
    "        doc.set_parser(parser)\n",
    "        doc.initialize() # optional if passwo is there: pdf_pwd\n",
    "\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # to use in-built pdftotxt.py:\n",
    "        #device = TextConverter(rsrcmgr, outfp, laparams=LAParams()) \n",
    "        #process_pdf(rsrcmgr, device, fp) \n",
    "        #device.close() \n",
    "\n",
    "        text_parsed = [] # a list of strings, each representing text collected from each page of the doc\n",
    "\n",
    "        # find the number of pages- starts from 0\n",
    "        num_pages = max([i for i,page in enumerate(doc.get_pages())]) + 1\n",
    "\n",
    "        look_for_outline = 1\n",
    "\n",
    "        # find the page with content outline (not defined as outline in the doc, unfortnately)\n",
    "        for i, page in enumerate(doc.get_pages()):\n",
    "            try:\n",
    "                   interpreter.process_page(page)\n",
    "            except Exception as e:\n",
    "                    print(\"parsing of the first page is not possible\")\n",
    "                    continue\n",
    "            # receive the LTPage object for this page\n",
    "            #The layout from get_result() parses the strings into separate objects. \n",
    "            #These objects have several key components:type, the coordinates \n",
    "            #(startingx, startingy, endingx, endingy), and the content, e.g.\n",
    "            #<LTRect 258.000,39.720,297.000,51.000>\n",
    "            #Accessing the type: e.g. type(object)==LTRect)\n",
    "            \n",
    "            layout = device.get_result()\n",
    "\n",
    "            objstack=list(reversed(layout._objs))\n",
    "\n",
    "            # get the first page info: type of gazette, vol \n",
    "            if i==0:\n",
    "                objstack_1 = objstack\n",
    "                continue\n",
    "            \n",
    "            # header from the 2nd page (any 2+ page would do)\n",
    "            if i==1:\n",
    "                objstack_2 = objstack\n",
    "                classification_data = get_classification_data(objstack_1, objstack_2)\n",
    "                continue\n",
    "\n",
    "            if(look_for_outline):\n",
    "                # find page with the outline and extract outline text and pages it points to:\n",
    "                is_outline_page, outline_obj, page_box, gazette_box = get_outline(objstack)\n",
    "                if (is_outline_page):\n",
    "                    #print(is_outline_page, i)\n",
    "                    num_outline_page=i\n",
    "                    # we found the outline so move to extracting pages from there\n",
    "                    look_for_outline = 0\n",
    "                    pages_to_extract, notice_info = get_notice_pages(page_box, gazette_box, num_pages)\n",
    "                    break\n",
    "             #else:\n",
    "              #  print(look_for_outline)\n",
    "             #now we have to fetch the columns corresponding to the 'page no' or 'page' \n",
    "             #and extract the page numbers the main info is stored at\n",
    "        device.close()\n",
    "        \n",
    "        return classification_data, pages_to_extract, notice_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'18 march 2016', 'government gazette', 'no. 39837', 'vol. 609'}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(classification_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refers to the document itself:\n",
    "class Issue:\n",
    "  def __init__(self, publication = '', issn = 0, num_pages = 0, volume=0, gazette_title=''):\n",
    "    self.publication = publication # type of gazette\n",
    "    self.identifier = issn # ISSN code\n",
    "    self.page_range = num_pages # number of pages\n",
    "    self.edition_id = volume # volume\n",
    "    self.title = gazette_title # if extraordinary\n",
    "\n",
    "# refers to notices where the relevant data is stored: info stored in pages \n",
    "#referred to in the outline + extra info on the type of data and its place on the web \n",
    "\n",
    "class Document:\n",
    "  def __init__(self, page_range = [] , uri = 'http://www.gpwonline.co.za', media_type = 'text' ): \n",
    "    self.page_range = page_range\n",
    "    self.url = uri\n",
    "    self.media_type =  media_type # {text, jpg, ...} -> some notices are Figures   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vol. 609', 'no. 39837', 'no. 39837', 'government gazette', '18 march 2016']"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we put the information into structured json template containing\n",
    "document identifiers of all sorts (classification of a gazette).\n",
    "\n",
    "Field ['subjects']: the entities that the notice is primarily about.\n",
    "\n",
    "Field ['about'] is for the content of the notice (parsed info).\n",
    "Depending on the type of document, it will either be \n",
    "\n",
    "- empty (if notices come as jpgs and cannot be processed), \n",
    "- csv/table for Liquor \n",
    "- tagged text for ... plain text\n",
    "\n",
    "\n",
    "Below we use json.dump to create a human readbale json format.\n",
    "However, for large no of files with large fileds and efficient way of saving data is:\n",
    "\n",
    "import jsonpickle\n",
    "json_obj = jsonpickle.encode(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_to_json(classification_data, pages_to_extract, notice_info, classification):\n",
    "        \"\"\"\n",
    "        classification_data: data about the document (date, vol, number, title)\n",
    "        pages_to_extract: where the info is\n",
    "        notice_info: what the notice is about, fetched from the 'outline'\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        classification = defaultdict(list) # dict to store data and dump into json\n",
    "\n",
    "        classification['issue'] = Issue() # instance of Issue\n",
    "        # publication = '', issn = 0, num_pages = 0, volume=0, notice_title='')\n",
    "\n",
    "        classification['document'] = Document() # instance of Document- mainly to gather\n",
    "           # pages where notices are published and their types\n",
    "\n",
    "        for x in set(classification_data):\n",
    "            if ('vol' in x):\n",
    "                uid_vol = x.split('.')[1].strip()\n",
    "\n",
    "                classification['issue'].edition_id = uid_vol\n",
    "\n",
    "            if ('no' in x):\n",
    "                uid_no = x.split('.')[1].strip()\n",
    "                classification['other_attributes'].append(uid_no) \n",
    "\n",
    "            if ('gazette' in x):\n",
    "                classification['issue'].publication = x\n",
    "                uid_type = x\n",
    "\n",
    "            if ('province' in x):\n",
    "                classification['other_attributes'].append(x)    \n",
    "\n",
    "            if ('extraordinary' in x):\n",
    "                classification['issue'].title = 'extraordinary'\n",
    "\n",
    "            if ('issn' in x):\n",
    "                classification['issue'].identifier = x\n",
    "            \n",
    "            #date\n",
    "            mult_dots = re.compile(r'(.){2,}') # 2 or more\n",
    "            #no = mult_dots.search(text_obj)\n",
    "            \n",
    "       \n",
    "            if ('december' in x):\n",
    "                print('left', x)\n",
    "                classification['date_published'] = x\n",
    "\n",
    "        # add info from the outline (keywords are important)\n",
    "        # shoulf be parsed: save keywords?\n",
    "        classification['summary'] = notice_info\n",
    "\n",
    "        # page_range, string /^[0-9]*(-[0-9]*)?$/\n",
    "        # The pages the document within the issue where to look for info\n",
    "\n",
    "        classification['document'].page_range =  pages_to_extract \n",
    "        classification['subjects'] = [] # entities\n",
    "        classification['about'] = [] # parsed info\n",
    "\n",
    "        classification['issue'].page_range = num_pages\n",
    "        #classification['source_url'] = \n",
    "\n",
    "        uid = uid_no + '_' + uid_vol\n",
    "        # modify id's if necessary\n",
    "        classification['uid'] = uid # must be unique\n",
    "        classification['identifier'] = uid  # + uid_type ? can be more descriptive possibly\n",
    "        \n",
    "        # see comment above this function\n",
    "        return json.dumps(classification, default=lambda o: o.__dict__, sort_keys=True)#, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left 31 december 2015\n"
     ]
    }
   ],
   "source": [
    "# open the pdf file\n",
    "fp = open('39569_31-12_NationalRegulation.pdf', 'rb')\n",
    "classification_data, pages_to_extract, notice_info = pdf_to_gazette_classification(fp)\n",
    "fp.close()\n",
    "\n",
    "classification_json = save_to_json(classification_data, pages_to_extract, notice_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regulation gazette',\n",
       " 'no. 10544',\n",
       " 'vol. 606',\n",
       " 'no. 39569',\n",
       " 'issn 1682-5843',\n",
       " 'no. 39569',\n",
       " 'government gazette',\n",
       " '31 december 2015']"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"date_published\": \"31 december 2015\", \"document\": {\"media_type\": \"text\", \"page_range\": [4], \"url\": \"http://www.gpwonline.co.za\"}, \"identifier\": \"39569_606\", \"issue\": {\"edition_id\": \"606\", \"identifier\": \"issn 1682-5843\", \"page_range\": 8, \"publication\": \"regulation gazette\", \"title\": \"\"}, \"other_attributes\": [\"10544\", \"39569\"], \"subjects\": [], \"summary\": [\"higher education and training, department of/ ho\\\\u00ebr onderwys en opleiding, departement van 336  skills development act (97/1998): correction notice: establish, alternatively re-establishment of sector education  and training authorities (setas)\"], \"uid\": \"39569_606\"}'"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "classification = defaultdict(list)\n",
    "\n",
    "classification['issue'] = Issue() # instance of Issue\n",
    "# publication = '', issn = 0, num_pages = 0, volume=0, notice_title='')\n",
    "\n",
    "classification['document'] = Document() # instance of Document- mainly to gather\n",
    "   # pages where notices are published and their types\n",
    "\n",
    "for x in set(classification_data):\n",
    "    if ('vol' in x):\n",
    "        uid_vol = x.split('.')[1].strip()\n",
    "        \n",
    "        classification['issue'].edition_id = x[1]\n",
    "    \n",
    "    if ('no' in x):\n",
    "        uid_no = x.split('.')[1].strip()\n",
    "        classification['other_attributes'].append(uid_no) \n",
    "    \n",
    "    if ('gazette' in x):\n",
    "        classification['issue'].publication = x\n",
    "        uid_type = x\n",
    "    \n",
    "    if ('province' in x):\n",
    "        print(x)\n",
    "        classification['other_attributes'].append(x)    \n",
    "    \n",
    "    if ('extraordinary' in x):\n",
    "        classification['issue'].title = 'extraordinary'\n",
    "            \n",
    "    if ('issue' in x):\n",
    "        classification['issue'].identifier = x\n",
    "        \n",
    "    #date\n",
    "    mult_dots = re.compile(r'(.){2,}') # 2 or more\n",
    "    #no = mult_dots.search(text_obj)\n",
    "    \n",
    "        classification['date_published'] = x\n",
    "                \n",
    "# add info from the outline (keywords are important)\n",
    "classification['summary'] = notice_info\n",
    "\n",
    "# page_range, string /^[0-9]*(-[0-9]*)?$/\n",
    "# The pages the document within the issue where to look for info\n",
    "\n",
    "classification['document'].pages_range =  pages_to_extract \n",
    "classification['subjects'] = [] # parsed notice info\n",
    "\n",
    "\n",
    "classification['issue'].num_pages = num_pages\n",
    "#classification['source_url'] = \n",
    "\n",
    "uid = '.'\n",
    "# modify id's if necessary\n",
    "classification['uid'] = uid # must be unique\n",
    "classification['identifier'] = uid  # + uid_type ? can be more descriptive possibly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18 march 2016'"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "match=re.search(r'(\\d+/*/\\d+)','The  11/12/98')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- detect whether it's a single or double column text\n",
    "- Extract just left column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing images:\n",
    "\n",
    "LTImage type: contains bits, colorspace, height,imagemask,name,srcsize,stream, and width.\n",
    "Note: PDFMiner seems work well only with jpegs ( apparently xpdf works with all images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if type(b) == LTImage:\n",
    "     imgbits=b.bits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "#result = es.suggest(index=\"gazettes\", body={\"my_suggestion\": {\"text\": notice, \"term\": {\"field\":\"content\" }}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
